{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e91ff08-ec1f-4178-81db-9634a66f5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Wings R Us Recommendation System\n",
    "# =======================================================================\n",
    "# This script implements a two-stage recommendation system for Wings R Us.\n",
    "# Stage 1: Candidate Generation - Generates a list of potential items using weighted signals.\n",
    "# Stage 2: Ranking - Uses LightGBM to rank candidates based on features.\n",
    "#\n",
    "# Key Design Choices:\n",
    "# - Training: Uses ALL past months for comprehensive learning.\n",
    "# - Validation: Uses the latest month to simulate real-world \"future\" performance.\n",
    "# - Features: Focus on customer segments (e.g., Guest, Registered) for efficiency.\n",
    "# - No date/time features: Ensures compatibility with test data that lacks them.\n",
    "# - Manual encoding: Avoids pd.get_dummies() for explicit control.\n",
    "# - Segment-based stats: Lightweight alternative to per-user computations.\n",
    "#\n",
    "# How to Run:\n",
    "# 1. Ensure data files (order_data.csv, store_data.csv, customer_data.csv, test_data_question.csv) are in the working directory.\n",
    "# 2. Install requirements: pandas, numpy, lightgbm, tqdm.\n",
    "# 3. Run the script: python this_file.py\n",
    "#\n",
    "# Output:\n",
    "# - Prints training/validation results and Recall@3.\n",
    "# - Generates TeamName_Recommendation_Output.csv for competition submission.\n",
    "#\n",
    "# For GitHub Hosting:\n",
    "# - Create a repository named \"wings-r-us-recommendation\".\n",
    "# - Add this file as main.py.\n",
    "# - Add a README.md with setup instructions and explanation.\n",
    "# - Include requirements.txt: pandas\\nnumpy\\nlightgbm\\ntqdm\n",
    "#\n",
    "# Author: Perplexity AI Assistant\n",
    "# Date: August 11, 2025\n",
    "# =======================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------------------------------------------------\n",
    "# Random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Training parameters\n",
    "NEG_PER_POS = 15  # Number of negative samples per positive example\n",
    "TIME_DECAY = 0.99  # Decay factor for recency weighting\n",
    "USER_AFF_TOPN = 25  # Top N items from user history for candidate generation\n",
    "SEGMENT_TOPN = 50  # Top N items per segment for candidate generation\n",
    "TOPK_CANDIDATES = 30  # Maximum candidates to generate per cart\n",
    "\n",
    "# Candidate generation weights (higher weight = stronger signal)\n",
    "W_COOC = 1.0      # Co-occurrence weight\n",
    "W_USER = 0.8      # User history weight\n",
    "W_STORE = 0.5     # Store popularity weight\n",
    "W_OCC = 0.4       # Occasion popularity weight\n",
    "W_STYPE = 0.2     # Store type popularity weight\n",
    "W_CTYPE = 0.2     # Customer type (segment) popularity weight\n",
    "W_GLOBAL = 0.1    # Global popularity fallback weight\n",
    "\n",
    "# Regional mapping dictionary\n",
    "REGION_MAP = {\n",
    "    'AZ': 'West', 'CA': 'West', 'CO': 'West', 'HI': 'West', 'NV': 'West',\n",
    "    'FL': 'South', 'NC': 'South', 'OK': 'South', 'TN': 'South', 'TX': 'South',\n",
    "    'IL': 'North', 'MI': 'North', 'NE': 'North', \n",
    "    'NJ': 'East', 'OT': 'Other'\n",
    "}\n",
    "\n",
    "# Categories for manual encoding\n",
    "CUSTOMER_TYPE_CATEGORIES = ['Guest', 'Registered', 'eClub', 'Online', 'Deleted Account']\n",
    "REGION_CATEGORIES = ['West', 'South', 'North', 'East', 'Other']\n",
    "OCCASION_CATEGORIES = ['Delivery', 'ToGo']\n",
    "\n",
    "print(\"üöÄ Wings R Us Customer Segment-Based Recommendation System\")\n",
    "print(\"üéØ ALL PAST MONTHS TRAINING + LATEST MONTH VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 1. DATA LOADING & PREPROCESSING\n",
    "# -----------------------------------------------------------------------\n",
    "# Load the three main CSV files\n",
    "print(\"üìÅ Loading and preprocessing data...\")\n",
    "orders = pd.read_csv('order_data.csv')\n",
    "stores = pd.read_csv('store_data.csv')\n",
    "customers = pd.read_csv('customer_data.csv')\n",
    "\n",
    "# Convert date column to datetime format\n",
    "orders['ORDER_CREATED_DATE'] = pd.to_datetime(orders['ORDER_CREATED_DATE'])\n",
    "\n",
    "# Merge orders with store and customer data on common keys\n",
    "orders = orders.merge(stores, on='STORE_NUMBER', how='left')\n",
    "orders = orders.merge(customers, on='CUSTOMER_ID', how='left')\n",
    "\n",
    "# Fill missing values with defaults\n",
    "orders['CUSTOMER_TYPE'] = orders['CUSTOMER_TYPE'].fillna('Guest')\n",
    "orders['ORDER_OCCASION_NAME'] = orders['ORDER_OCCASION_NAME'].fillna('ToGo')\n",
    "if 'STORE_TYPE' not in orders.columns:\n",
    "    orders['STORE_TYPE'] = 'Standard'\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 2. MANUAL ENCODING (No Date/Time Features)\n",
    "# -----------------------------------------------------------------------\n",
    "# Create categorical features using manual one-hot encoding\n",
    "print(\"üè∑Ô∏è Creating manual customer segment and regional encoding...\")\n",
    "\n",
    "# Map states to regions\n",
    "orders['REGION'] = orders['STATE'].map(REGION_MAP).fillna('Other')\n",
    "\n",
    "# Manual one-hot encode CUSTOMER_TYPE\n",
    "for ctype in CUSTOMER_TYPE_CATEGORIES:\n",
    "    column_name = f\"customer_{ctype.lower().replace(' ', '_')}\"\n",
    "    orders[column_name] = (orders['CUSTOMER_TYPE'] == ctype).astype(int)\n",
    "\n",
    "# Manual one-hot encode REGION\n",
    "for region_type in REGION_CATEGORIES:\n",
    "    column_name = f\"region_{region_type.lower()}\"\n",
    "    orders[column_name] = (orders['REGION'] == region_type).astype(int)\n",
    "\n",
    "# Manual one-hot encode ORDER_OCCASION_NAME  \n",
    "for occasion_type in OCCASION_CATEGORIES:\n",
    "    column_name = f\"occasion_{occasion_type.lower()}\"\n",
    "    orders[column_name] = (orders['ORDER_OCCASION_NAME'] == occasion_type).astype(int)\n",
    "\n",
    "# Store column names for later use in feature extraction\n",
    "CUSTOMER_TYPE_COLS = [f\"customer_{ctype.lower().replace(' ', '_')}\" for ctype in CUSTOMER_TYPE_CATEGORIES]\n",
    "REGION_COLS = [f\"region_{region.lower()}\" for region in REGION_CATEGORIES]\n",
    "OCCASION_COLS = [f\"occasion_{occasion.lower()}\" for occasion in OCCASION_CATEGORIES]\n",
    "\n",
    "# Drop the temporary REGION column\n",
    "orders = orders.drop(['REGION'], axis=1)\n",
    "\n",
    "print(f\"   ‚úÖ Added {len(CUSTOMER_TYPE_COLS)} customer segment features\")\n",
    "print(f\"   ‚úÖ Added {len(OCCASION_COLS)} occasion features and {len(REGION_COLS)} region features\")\n",
    "print(f\"   üö´ Removed all date/time features for test data compatibility\")\n",
    "\n",
    "# Parse the JSON in the ORDERS column to extract item lists\n",
    "def parse_items(j):\n",
    "    out = []\n",
    "    try:\n",
    "        obj = json.loads(j)\n",
    "        for blk in obj.get('orders', []):\n",
    "            for d in blk.get('item_details', []):\n",
    "                if float(d.get('item_price', 0)) > 0:\n",
    "                    qty = int(d.get('item_quantity', 1) or 1)\n",
    "                    out.extend([d['item_name']] * qty)\n",
    "    except:\n",
    "        pass\n",
    "    return out\n",
    "\n",
    "orders['ITEM_LIST'] = orders['ORDERS'].apply(parse_items)\n",
    "orders = orders[orders['ITEM_LIST'].str.len() > 0].reset_index(drop=True)\n",
    "orders = orders.sort_values('ORDER_CREATED_DATE').reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(orders):,} orders\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 3. ALL PAST MONTHS TRAINING + LATEST MONTH VALIDATION SPLIT\n",
    "# -----------------------------------------------------------------------\n",
    "# Assign monthly periods for splitting\n",
    "print(\"üìÖ Creating ALL PAST MONTHS training + LATEST MONTH validation split...\")\n",
    "\n",
    "orders['YEAR_MONTH'] = orders['ORDER_CREATED_DATE'].dt.to_period('M')\n",
    "months = sorted(orders['YEAR_MONTH'].unique())\n",
    "\n",
    "if len(months) < 2:\n",
    "    print(f\"‚ö†Ô∏è Only {len(months)} month(s) available, cannot create proper split\")\n",
    "    raise SystemExit\n",
    "\n",
    "# Latest month for validation, all previous for training\n",
    "val_month = months[-1]\n",
    "train_months = months[:-1]\n",
    "\n",
    "val_df = orders[orders['YEAR_MONTH'] == val_month].copy()\n",
    "train_df = orders[orders['YEAR_MONTH'].isin(train_months)].copy()\n",
    "\n",
    "print(f\"üìä Training months: {len(train_months)} months from {train_months[0]} to {train_months[-1]} ({len(train_df):,} orders)\")\n",
    "print(f\"üìä Validation month: {val_month} ({len(val_df):,} orders)\")\n",
    "print(f\"üéØ Using ALL {len(train_months)} historical months for training\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 4. SEGMENT-BASED Feature Building (No User-Level Stats)\n",
    "# -----------------------------------------------------------------------\n",
    "# Dictionary to cache features for each month\n",
    "print(\"üèóÔ∏è Building SEGMENT-BASED features from historical data...\")\n",
    "\n",
    "feature_cache = {}\n",
    "\n",
    "# Helper function to get cutoff date (last day of previous month)\n",
    "def get_previous_month_end(order_date):\n",
    "    first_day = order_date.replace(day=1)\n",
    "    return (first_day - timedelta(days=1)).date()\n",
    "\n",
    "# Function to build features up to a cutoff date\n",
    "def build_segment_based_features(cutoff_date):\n",
    "    hist_df = orders[orders['ORDER_CREATED_DATE'].dt.date <= cutoff_date].copy()\n",
    "    if len(hist_df) == 0:\n",
    "        return None\n",
    "    \n",
    "    print(f\"   üìà Building segment-based features from {len(hist_df):,} orders (up to {cutoff_date})\")\n",
    "    \n",
    "    # Global item statistics\n",
    "    all_items = [item for lst in hist_df['ITEM_LIST'] for item in lst]\n",
    "    item_counts = Counter(all_items)\n",
    "    total_baskets = len(hist_df)\n",
    "    global_support = {item: count/total_baskets for item, count in item_counts.items()}\n",
    "    \n",
    "    # Simple user recency affinity (keep this lightweight)\n",
    "    recency_baseline = hist_df['ORDER_CREATED_DATE'].max()\n",
    "    user_affinity = defaultdict(Counter)\n",
    "    for row in hist_df[['CUSTOMER_ID', 'ORDER_CREATED_DATE', 'ITEM_LIST']].itertuples(index=False):\n",
    "        cid, date, items = row\n",
    "        decay = TIME_DECAY ** (recency_baseline - date).days\n",
    "        for item in items:\n",
    "            user_affinity[cid][item] += decay\n",
    "    \n",
    "    # Enhanced SEGMENT-based popularity\n",
    "    segment_pop = {}\n",
    "    store_pop = defaultdict(Counter)\n",
    "    occ_pop = defaultdict(Counter)\n",
    "    region_pop = defaultdict(Counter)\n",
    "    \n",
    "    # CUSTOMER SEGMENT statistics\n",
    "    for segment in CUSTOMER_TYPE_CATEGORIES:\n",
    "        segment_orders = hist_df[hist_df['CUSTOMER_TYPE'] == segment]\n",
    "        if len(segment_orders) > 0:\n",
    "            segment_items = Counter(item for order_items in segment_orders['ITEM_LIST'] for item in order_items)\n",
    "            segment_pop[segment] = dict(segment_items)\n",
    "    \n",
    "    for row in hist_df.itertuples():\n",
    "        # Extract region from one-hot columns\n",
    "        region = 'Other'\n",
    "        for col in REGION_COLS:\n",
    "            if hasattr(row, col) and getattr(row, col) == 1:\n",
    "                region = col.replace('region_', '').title()\n",
    "                break\n",
    "                \n",
    "        for item in row.ITEM_LIST:\n",
    "            store_pop[row.STORE_NUMBER][item] += 1\n",
    "            occ_pop[row.ORDER_OCCASION_NAME][item] += 1\n",
    "            region_pop[region][item] += 1\n",
    "    \n",
    "    # Denominators\n",
    "    store_counts = hist_df['STORE_NUMBER'].value_counts().to_dict()\n",
    "    segment_counts = hist_df['CUSTOMER_TYPE'].value_counts().to_dict()\n",
    "    occ_counts = hist_df['ORDER_OCCASION_NAME'].value_counts().to_dict()\n",
    "    region_counts = {'West': 1, 'South': 1, 'North': 1, 'East': 1, 'Other': 1}\n",
    "    \n",
    "    # Global co-occurrence matrix\n",
    "    co_occurrence = defaultdict(Counter)\n",
    "    for basket in hist_df['ITEM_LIST']:\n",
    "        unique_items = list(set(basket))\n",
    "        for i in range(len(unique_items)):\n",
    "            for j in range(i+1, len(unique_items)):\n",
    "                a, b = unique_items[i], unique_items[j]\n",
    "                co_occurrence[a][b] += 1\n",
    "                co_occurrence[b][a] += 1\n",
    "    \n",
    "    # Global confidence matrix\n",
    "    global_confidence_matrix = defaultdict(dict)\n",
    "    for item_a, neighbors in co_occurrence.items():\n",
    "        count_a = item_counts.get(item_a, 1)\n",
    "        for item_b, cooccur_count in neighbors.items():\n",
    "            if item_a != item_b:\n",
    "                global_confidence_matrix[item_a][item_b] = cooccur_count / count_a\n",
    "    \n",
    "    # SEGMENT-LEVEL confidence statistics (instead of user-level)\n",
    "    segment_confidence_stats = {}\n",
    "    for segment in CUSTOMER_TYPE_CATEGORIES:\n",
    "        if segment in segment_counts and segment_counts[segment] > 5:  # Only if enough data\n",
    "            segment_items = Counter(item for order_items in hist_df[hist_df['CUSTOMER_TYPE'] == segment]['ITEM_LIST'] \n",
    "                                  for item in order_items)\n",
    "            top_segment_items = [item for item, _ in segment_items.most_common(15)]\n",
    "            \n",
    "            confidences = []\n",
    "            for item_a in top_segment_items:\n",
    "                for item_b in top_segment_items:\n",
    "                    if item_a != item_b and item_b in global_confidence_matrix.get(item_a, {}):\n",
    "                        confidences.append(global_confidence_matrix[item_a][item_b])\n",
    "            \n",
    "            segment_confidence_stats[segment] = {\n",
    "                'avg': np.mean(confidences) if confidences else 0.0,\n",
    "                'max': np.max(confidences) if confidences else 0.0\n",
    "            }\n",
    "        else:\n",
    "            segment_confidence_stats[segment] = {'avg': 0.0, 'max': 0.0}\n",
    "    \n",
    "    return {\n",
    "        'global_support': global_support,\n",
    "        'user_affinity': dict(user_affinity),\n",
    "        'global_confidence_matrix': dict(global_confidence_matrix),\n",
    "        'segment_pop': segment_pop,\n",
    "        'segment_confidence_stats': segment_confidence_stats,\n",
    "        'segment_counts': segment_counts,\n",
    "        'item_counts': dict(item_counts),\n",
    "        'store_pop': dict(store_pop),\n",
    "        'occ_pop': dict(occ_pop),\n",
    "        'region_pop': dict(region_pop),\n",
    "        'store_counts': store_counts,\n",
    "        'occ_counts': occ_counts,\n",
    "        'region_counts': region_counts\n",
    "    }\n",
    "\n",
    "# Build features for train and validation months\n",
    "# MODIFIED: Build features for ALL training months + validation month\n",
    "all_months_to_cache = list(train_months) + [val_month]\n",
    "\n",
    "for month in all_months_to_cache:\n",
    "    month_start = month.to_timestamp()\n",
    "    cutoff_date = get_previous_month_end(month_start)\n",
    "    features = build_segment_based_features(cutoff_date)\n",
    "    if features:\n",
    "        feature_cache[str(month)] = features\n",
    "        print(f\"   ‚úÖ Cached segment-based features for {month} (cutoff: {cutoff_date})\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 5. Candidate Generation\n",
    "# -----------------------------------------------------------------------\n",
    "# Function to generate candidates using weighted signals\n",
    "print(\"Generating candidates...\")\n",
    "def generate_candidates(cart_items, cid, ctype, store, occ, region, feature_data):\n",
    "    if not feature_data:\n",
    "        return []\n",
    "    \n",
    "    candidates = Counter()\n",
    "    \n",
    "    # 1. Co-occurrence\n",
    "    for item in cart_items:\n",
    "        for neighbor, conf in feature_data['global_confidence_matrix'].get(item, {}).items():\n",
    "            if neighbor not in cart_items:\n",
    "                candidates[neighbor] += W_COOC * conf\n",
    "    \n",
    "    # 2. User affinity\n",
    "    if cid in feature_data['user_affinity']:\n",
    "        for item, score in Counter(feature_data['user_affinity'][cid]).most_common(USER_AFF_TOPN):\n",
    "            if item not in cart_items:\n",
    "                candidates[item] += W_USER * score\n",
    "    \n",
    "    # 3. Store popularity\n",
    "    if store in feature_data['store_pop']:\n",
    "        for item, count in Counter(feature_data['store_pop'][store]).most_common(SEGMENT_TOPN):\n",
    "            if item not in cart_items:\n",
    "                candidates[item] += W_STORE * count / feature_data['store_counts'].get(store, 1)\n",
    "    \n",
    "    # 4. Customer SEGMENT popularity\n",
    "    if ctype in feature_data['segment_pop']:\n",
    "        for item, count in Counter(feature_data['segment_pop'][ctype]).most_common(SEGMENT_TOPN):\n",
    "            if item not in cart_items:\n",
    "                candidates[item] += W_CTYPE * count / feature_data['segment_counts'].get(ctype, 1)\n",
    "    \n",
    "    # 5. Occasion and region popularity\n",
    "    segments = [\n",
    "        (feature_data['occ_pop'].get(occ, {}), W_OCC, feature_data['occ_counts'].get(occ, 1)),\n",
    "        (feature_data['region_pop'].get(region, {}), 0.3, feature_data['region_counts'].get(region, 1))\n",
    "    ]\n",
    "    \n",
    "    for pop_dict, weight, denominator in segments:\n",
    "        for item, count in Counter(pop_dict).most_common(SEGMENT_TOPN):\n",
    "            if item not in cart_items:\n",
    "                candidates[item] += weight * count / denominator\n",
    "    \n",
    "    # 6. Global popularity\n",
    "    for item, score in feature_data['global_support'].items():\n",
    "        if item not in cart_items:\n",
    "            candidates[item] += W_GLOBAL * score\n",
    "    \n",
    "    return [item for item, _ in candidates.most_common(TOPK_CANDIDATES)]\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 6. SEGMENT-FOCUSED Feature Extraction\n",
    "# -----------------------------------------------------------------------\n",
    "# Function to extract features for ranking, focused on segments\n",
    "print(\"Extracting features...\")\n",
    "def extract_segment_features(customer_id, input_cart, candidate_item, feature_data, order_context=None):\n",
    "    \"\"\"Extract segment-focused features only\"\"\"\n",
    "    if not feature_data:\n",
    "        return {}\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Core global features\n",
    "    features['global_support'] = feature_data['global_support'].get(candidate_item, 0.0)\n",
    "    features['user_recency_score'] = feature_data['user_affinity'].get(customer_id, {}).get(candidate_item, 0.0)\n",
    "    features['cart_size'] = len(input_cart)\n",
    "    \n",
    "    # Global cart-based association features\n",
    "    global_conf_scores = [\n",
    "        feature_data['global_confidence_matrix'].get(cart_item, {}).get(candidate_item, 0.0) \n",
    "        for cart_item in input_cart \n",
    "        if cart_item != candidate_item\n",
    "    ]\n",
    "    \n",
    "    features['global_avg_confidence_from_cart'] = float(np.mean(global_conf_scores)) if global_conf_scores else 0.0\n",
    "    features['global_max_confidence_from_cart'] = float(np.max(global_conf_scores)) if global_conf_scores else 0.0\n",
    "    \n",
    "    # Item classification features\n",
    "    features['is_dip'] = int('dip' in candidate_item.lower())\n",
    "    features['is_combo'] = int('combo' in candidate_item.lower())\n",
    "    features['is_wings'] = int('wings' in candidate_item.lower())\n",
    "    features['is_sides'] = int(any(side in candidate_item.lower() for side in ['fries', 'corn', 'sticks']))\n",
    "    features['is_strips'] = int('strips' in candidate_item.lower())\n",
    "    \n",
    "    if order_context:\n",
    "        # Customer SEGMENT features (instead of individual user stats)\n",
    "        for col in CUSTOMER_TYPE_COLS:\n",
    "            features[col] = order_context.get(col, 0)\n",
    "        \n",
    "        # SEGMENT-based confidence features\n",
    "        customer_segment = order_context.get('CUSTOMER_TYPE', 'Guest')\n",
    "        features['segment_avg_confidence'] = feature_data['segment_confidence_stats'].get(customer_segment, {}).get('avg', 0.0)\n",
    "        features['segment_max_confidence'] = feature_data['segment_confidence_stats'].get(customer_segment, {}).get('max', 0.0)\n",
    "        \n",
    "        # Regional features\n",
    "        for col in REGION_COLS:\n",
    "            features[col] = order_context.get(col, 0)\n",
    "        \n",
    "        # Occasion features  \n",
    "        for col in OCCASION_COLS:\n",
    "            features[col] = order_context.get(col, 0)\n",
    "        \n",
    "        # Cart composition features\n",
    "        unique_cart = set(input_cart)\n",
    "        categories = set()\n",
    "        for item in unique_cart:\n",
    "            if 'wings' in item.lower():\n",
    "                categories.add('wings')\n",
    "            elif 'combo' in item.lower():\n",
    "                categories.add('combo')\n",
    "            elif 'dip' in item.lower():\n",
    "                categories.add('dip')\n",
    "            elif 'fries' in item.lower():\n",
    "                categories.add('sides')\n",
    "        features['category_diversity'] = len(categories) / 4\n",
    "        \n",
    "        # Store popularity for this item\n",
    "        store_items = feature_data['store_pop'].get(order_context.get('STORE_NUMBER', {}), {})\n",
    "        features['store_popularity'] = store_items.get(candidate_item, 0) / max(sum(store_items.values()), 1)\n",
    "        \n",
    "        # Segment popularity for this item\n",
    "        segment_items = feature_data['segment_pop'].get(customer_segment, {})\n",
    "        features['segment_popularity'] = segment_items.get(candidate_item, 0) / max(sum(segment_items.values()), 1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 7. Training Data Generation - ALL PAST MONTHS\n",
    "# -----------------------------------------------------------------------\n",
    "# Generate training data using leave-one-out with negative sampling\n",
    "print(\"üéØ Creating segment-based training data from ALL PAST MONTHS...\")\n",
    "\n",
    "all_feature_rows = []\n",
    "all_labels = []\n",
    "\n",
    "# Process ALL training months\n",
    "for month in train_months:\n",
    "    month_data = train_df[train_df['YEAR_MONTH'] == month]\n",
    "    feature_data = feature_cache.get(str(month))\n",
    "    \n",
    "    if not feature_data:\n",
    "        print(f\"   ‚ö†Ô∏è No feature data for {month}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"   Processing {month} ({len(month_data)} orders)...\")\n",
    "    \n",
    "    for row in tqdm(month_data.itertuples(), desc=f\"Month {month}\"):\n",
    "        actual_items = set(row.ITEM_LIST)\n",
    "        if len(actual_items) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Extract customer segment and region\n",
    "        customer_type = row.CUSTOMER_TYPE\n",
    "        region = 'Other'\n",
    "        for col in REGION_COLS:\n",
    "            if hasattr(row, col) and getattr(row, col) == 1:\n",
    "                region = col.replace('region_', '').title()\n",
    "                break\n",
    "        \n",
    "        # Order context (NO DATE/TIME FEATURES)\n",
    "        order_context = {\n",
    "            'STORE_NUMBER': getattr(row, 'STORE_NUMBER', None),\n",
    "            'CUSTOMER_TYPE': customer_type,\n",
    "            **{col: getattr(row, col, 0) for col in OCCASION_COLS + REGION_COLS + CUSTOMER_TYPE_COLS}\n",
    "        }\n",
    "        \n",
    "        # Leave-one-out training\n",
    "        unique_items = list(actual_items)\n",
    "        for i, positive_item in enumerate(unique_items):\n",
    "            input_cart = unique_items[:i] + unique_items[i+1:]\n",
    "            \n",
    "            candidates = generate_candidates(\n",
    "                input_cart, row.CUSTOMER_ID, customer_type,\n",
    "                row.STORE_NUMBER, row.ORDER_OCCASION_NAME, region, feature_data\n",
    "            )\n",
    "            \n",
    "            if positive_item not in candidates:\n",
    "                candidates.append(positive_item)\n",
    "            \n",
    "            # Negative sampling\n",
    "            negative_candidates = [c for c in candidates if c != positive_item]\n",
    "            if len(negative_candidates) > NEG_PER_POS:\n",
    "                negative_candidates = np.random.choice(\n",
    "                    negative_candidates, size=NEG_PER_POS, replace=False\n",
    "                ).tolist()\n",
    "            \n",
    "            # Positive example\n",
    "            features = extract_segment_features(row.CUSTOMER_ID, input_cart, positive_item, feature_data, order_context)\n",
    "            all_feature_rows.append(features)\n",
    "            all_labels.append(1)\n",
    "            \n",
    "            # Negative examples\n",
    "            for neg_item in negative_candidates:\n",
    "                features = extract_segment_features(row.CUSTOMER_ID, input_cart, neg_item, feature_data, order_context)\n",
    "                all_feature_rows.append(features)\n",
    "                all_labels.append(0)\n",
    "\n",
    "# Create DataFrame from all collected features\n",
    "X_train = pd.DataFrame(all_feature_rows)\n",
    "y_train = pd.Series(all_labels, name='label')\n",
    "all_feature_columns = list(X_train.columns)\n",
    "\n",
    "print(f\"üìä Segment-based training data: {len(X_train):,} examples, {y_train.mean():.1%} positive\")\n",
    "print(f\"üéõÔ∏è Using {len(all_feature_columns)} features including segment-based statistics\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 8. Model Training\n",
    "# -----------------------------------------------------------------------\n",
    "# Train LightGBM model on the training data\n",
    "print(\"ü§ñ Training segment-focused LightGBM on full historical data...\")\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    random_state=SEED,\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=63,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    is_unbalance=True,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 9. Validation on Latest Month\n",
    "# -----------------------------------------------------------------------\n",
    "# Evaluate model on validation set using Recall@3\n",
    "print(\"üéØ Validating segment-based model on latest month...\")\n",
    "\n",
    "val_feature_data = feature_cache.get(str(val_month))\n",
    "hits = 0\n",
    "total = 0\n",
    "\n",
    "for row in tqdm(val_df.itertuples(), desc=\"Segment validation\"):\n",
    "    unique_cart = list(dict.fromkeys(row.ITEM_LIST))\n",
    "    if len(unique_cart) < 2:\n",
    "        continue\n",
    "    \n",
    "    # Single random item removal\n",
    "    target_idx = np.random.randint(0, len(unique_cart))\n",
    "    true_item = unique_cart[target_idx]\n",
    "    input_cart = unique_cart[:target_idx] + unique_cart[target_idx+1:]\n",
    "    total += 1\n",
    "    \n",
    "    # Extract customer segment and region\n",
    "    customer_type = row.CUSTOMER_TYPE\n",
    "    region = 'Other'\n",
    "    for col in REGION_COLS:\n",
    "        if hasattr(row, col) and getattr(row, col) == 1:\n",
    "            region = col.replace('region_', '').title()\n",
    "            break\n",
    "    \n",
    "    # Order context (NO DATE/TIME FEATURES)\n",
    "    order_context = {\n",
    "        'STORE_NUMBER': getattr(row, 'STORE_NUMBER', None),\n",
    "        'CUSTOMER_TYPE': customer_type,\n",
    "        **{col: getattr(row, col, 0) for col in OCCASION_COLS + REGION_COLS + CUSTOMER_TYPE_COLS}\n",
    "    }\n",
    "    \n",
    "    candidates = generate_candidates(\n",
    "        input_cart, row.CUSTOMER_ID, customer_type,\n",
    "        row.STORE_NUMBER, row.ORDER_OCCASION_NAME, region, val_feature_data\n",
    "    )\n",
    "    \n",
    "    if true_item not in candidates:\n",
    "        candidates.append(true_item)\n",
    "    \n",
    "    # Score candidates\n",
    "    candidate_features = []\n",
    "    valid_candidates = []\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        if candidate in input_cart:\n",
    "            continue\n",
    "        features = extract_segment_features(row.CUSTOMER_ID, input_cart, candidate, val_feature_data, order_context)\n",
    "        candidate_features.append(features)\n",
    "        valid_candidates.append(candidate)\n",
    "    \n",
    "    if candidate_features:\n",
    "        X_candidates = pd.DataFrame(candidate_features)\n",
    "        # Ensure all training columns exist\n",
    "        for col in all_feature_columns:\n",
    "            if col not in X_candidates.columns:\n",
    "                X_candidates[col] = 0\n",
    "        \n",
    "        X_candidates = X_candidates[all_feature_columns]\n",
    "        scores = model.predict_proba(X_candidates)[:, 1]\n",
    "        top_3_indices = np.argsort(scores)[-3:][::-1]\n",
    "        top_3_recs = [valid_candidates[i] for i in top_3_indices]\n",
    "        \n",
    "        if true_item in top_3_recs:\n",
    "            hits += 1\n",
    "\n",
    "recall_at_3 = hits / total if total > 0 else 0.0\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 10. Test Set Prediction\n",
    "# -----------------------------------------------------------------------\n",
    "# Generate predictions for test set if available\n",
    "try:\n",
    "    test_df = pd.read_csv('test_data_question.csv')\n",
    "    print(\"üèÜ Generating test predictions with segment-based model...\")\n",
    "    \n",
    "    # Ensure test data has required columns (add missing ones)\n",
    "    for col in CUSTOMER_TYPE_COLS + REGION_COLS + OCCASION_COLS:\n",
    "        if col not in test_df.columns:\n",
    "            test_df[col] = 0\n",
    "    \n",
    "    # Manual encoding for test data\n",
    "    if 'STATE' in test_df.columns:\n",
    "        test_df['REGION'] = test_df['STATE'].map(REGION_MAP).fillna('Other')\n",
    "        \n",
    "        # Encode customer types\n",
    "        for ctype in CUSTOMER_TYPE_CATEGORIES:\n",
    "            column_name = f\"customer_{ctype.lower().replace(' ', '_')}\"\n",
    "            if column_name not in test_df.columns:\n",
    "                test_df[column_name] = (test_df['CUSTOMER_TYPE'] == ctype).astype(int)\n",
    "        \n",
    "        # Encode regions\n",
    "        for region_type in REGION_CATEGORIES:\n",
    "            column_name = f\"region_{region_type.lower()}\"\n",
    "            if column_name not in test_df.columns:\n",
    "                test_df[column_name] = (test_df['REGION'] == region_type).astype(int)\n",
    "        \n",
    "        # Encode occasions\n",
    "        for occasion_type in OCCASION_CATEGORIES:\n",
    "            column_name = f\"occasion_{occasion_type.lower()}\"\n",
    "            if column_name not in test_df.columns:\n",
    "                test_df[column_name] = (test_df['ORDER_OCCASION_NAME'] == occasion_type).astype(int)\n",
    "    \n",
    "    predictions = []\n",
    "    latest_feature_data = feature_cache[str(val_month)]\n",
    "    \n",
    "    for row in tqdm(test_df.itertuples(), total=len(test_df), desc=\"Test predictions\"):\n",
    "        # Extract cart from test format\n",
    "        cart_items = []\n",
    "        for col in ['item1', 'item2', 'item3']:\n",
    "            if hasattr(row, col):\n",
    "                item = getattr(row, col)\n",
    "                if pd.notna(item) and str(item) != '':\n",
    "                    cart_items.append(str(item))\n",
    "        \n",
    "        if not cart_items:\n",
    "            predictions.append(['10 pc Spicy Wings', 'Ranch Dip - Regular', 'Regular Buffalo Fries'])\n",
    "            continue\n",
    "        \n",
    "        # Extract customer segment and region\n",
    "        customer_type = getattr(row, 'CUSTOMER_TYPE', 'Guest')\n",
    "        region = 'Other'\n",
    "        for col in REGION_COLS:\n",
    "            if hasattr(row, col) and getattr(row, col) == 1:\n",
    "                region = col.replace('region_', '').title()\n",
    "                break\n",
    "        \n",
    "        order_context = {\n",
    "            'STORE_NUMBER': getattr(row, 'STORE_NUMBER', None),\n",
    "            'CUSTOMER_TYPE': customer_type,\n",
    "            **{col: getattr(row, col, 0) for col in OCCASION_COLS + REGION_COLS + CUSTOMER_TYPE_COLS}\n",
    "        }\n",
    "        \n",
    "        candidates = generate_candidates(\n",
    "            cart_items, row.CUSTOMER_ID, customer_type,\n",
    "            row.STORE_NUMBER, row.ORDER_OCCASION_NAME, region, latest_feature_data\n",
    "        )\n",
    "        \n",
    "        # Score candidates\n",
    "        candidate_features = []\n",
    "        valid_candidates = []\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            if candidate not in cart_items:\n",
    "                features = extract_segment_features(row.CUSTOMER_ID, cart_items, candidate, latest_feature_data, order_context)\n",
    "                candidate_features.append(features)\n",
    "                valid_candidates.append(candidate)\n",
    "        \n",
    "        if candidate_features:\n",
    "            X_candidates = pd.DataFrame(candidate_features)\n",
    "            # Ensure all training columns exist\n",
    "            for col in all_feature_columns:\n",
    "                if col not in X_candidates.columns:\n",
    "                    X_candidates[col] = 0\n",
    "            \n",
    "            X_candidates = X_candidates[all_feature_columns]\n",
    "            scores = model.predict_proba(X_candidates)[:, 1]\n",
    "            \n",
    "            # Get top-3 recommendations\n",
    "            top_3_indices = np.argsort(scores)[-3:][::-1]\n",
    "            top_3_recs = [valid_candidates[i] for i in top_3_indices]\n",
    "        else:\n",
    "            top_3_recs = ['10 pc Spicy Wings', 'Ranch Dip - Regular', 'Regular Buffalo Fries']\n",
    "        \n",
    "        predictions.append(top_3_recs)\n",
    "    \n",
    "    # Save in competition format\n",
    "    output_df = test_df.copy()\n",
    "    \n",
    "    output_df['RECOMMENDATION_1'] = [pred[0] if len(pred) > 0 else '' for pred in predictions]\n",
    "    output_df['RECOMMENDATION_2'] = [pred[1] if len(pred) > 1 else '' for pred in predictions] \n",
    "    output_df['RECOMMENDATION_3'] = [pred[2] if len(pred) > 2 else '' for pred in predictions]\n",
    "    \n",
    "    # Correct column order\n",
    "    required_cols = [\n",
    "        'CUSTOMER_ID', 'STORE_NUMBER', 'ORDER_ID', 'ORDER_CHANNEL_NAME',\n",
    "        'ORDER_SUBCHANNEL_NAME', 'ORDER_OCCASION_NAME', 'CUSTOMER_TYPE',\n",
    "        'item1', 'item2', 'item3',\n",
    "        'RECOMMENDATION_1', 'RECOMMENDATION_2', 'RECOMMENDATION_3'\n",
    "    ]\n",
    "    \n",
    "    output_cols = [col for col in required_cols if col in output_df.columns]\n",
    "    final_output = output_df[output_cols]\n",
    "    \n",
    "    final_output.to_csv('TeamName_Recommendation_Output.csv', index=False)\n",
    "    print(f\"üíæ Saved competition output to TeamName_Recommendation_Output.csv\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è test_data_question.csv not found, skipping test predictions\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 11. Final Results Summary\n",
    "# -----------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ ENHANCED ALL-HISTORY RECOMMENDATION SYSTEM RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìà Enhanced Recall@3: {recall_at_3:.4f} ({hits:,}/{total:,})\")\n",
    "print(f\"üéõÔ∏è Total Features: {len(all_feature_columns)}\")\n",
    "print(f\"üìä Training: {len(train_df):,} orders from ALL {len(train_months)} historical months\")\n",
    "print(f\"üìä Validation: {len(val_df):,} orders from latest month {val_month}\")\n",
    "print(f\"üìà Training data improvement: {len(train_months)} months vs previous 2-month limit\")\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è Enhanced Training Coverage:\")\n",
    "print(f\"   üìÖ Historical months used: {len(train_months)} (from {train_months[0]} to {train_months[-1]})\")\n",
    "print(f\"   üìä Customer Segments: {CUSTOMER_TYPE_CATEGORIES}\")\n",
    "print(f\"   üåç Regions: {REGION_CATEGORIES}\")\n",
    "print(f\"   üì± Occasions: {OCCASION_CATEGORIES}\")\n",
    "print(f\"   üéØ Key Features: segment_avg_confidence, segment_max_confidence, segment_popularity\")\n",
    "\n",
    "# Feature importance\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    print(\"\\nüìä Top 15 Feature Importance:\")\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': all_feature_columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    for _, row in importance_df.head(15).iterrows():\n",
    "        print(f\"   {row.feature}: {row.importance:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Enhanced system using ALL available historical data for training\")\n",
    "print(f\"‚úÖ Validation on most recent month for realistic performance assessment\")\n",
    "print(f\"‚úÖ Customer segment-based features optimized for production deployment\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
